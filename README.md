# ğŸŒ¡ï¸ PredicciÃ³n de Temperatura con Red Neuronal desde Cero

ImplementaciÃ³n completa de una **Red Neuronal con Backpropagation** para predecir temperaturas mensuales usando datos meteorolÃ³gicos de NASA POWER.

---

## ğŸ“‹ DescripciÃ³n del Proyecto

Este proyecto implementa una red neuronal **completamente desde cero** (sin usar librerÃ­as de deep learning como TensorFlow o PyTorch) para predecir la temperatura promedio del mes siguiente basÃ¡ndose en datos histÃ³ricos de temperatura.

### âœ¨ CaracterÃ­sticas

- âœ… Backpropagation implementado manualmente
- âœ… Utiliza datos reales de NASA POWER (SudamÃ©rica, 1984-2022)
- âœ… Incluye tres variables: T2M, T2M_MAX, T2M_MIN
- âœ… Visualizaciones completas de resultados
- âœ… ComparaciÃ³n con baselines
- âœ… AnÃ¡lisis de errores
- âœ… Interfaz interactiva para predicciones

---

## ğŸ—ï¸ Arquitectura de la Red

```
ENTRADA (9 neuronas)
  â†“
[T2M, MAX, MIN] Ã— 3 meses
  â†“
CAPA OCULTA (16 neuronas)
  â†“
ActivaciÃ³n: ReLU
  â†“
CAPA SALIDA (1 neurona)
  â†“
PredicciÃ³n: T2M mes siguiente
```

### Componentes Implementados

1. **Forward Propagation**: CÃ¡lculo de predicciones
2. **Backward Propagation**: CÃ¡lculo de gradientes usando regla de la cadena
3. **Gradient Descent**: ActualizaciÃ³n de pesos
4. **Mini-Batch Training**: Entrenamiento eficiente por lotes
5. **NormalizaciÃ³n**: EstandarizaciÃ³n de datos

---

## ğŸ“‚ Estructura del Proyecto

```
proyecto_backpropagation/
â”‚
â”œâ”€â”€ data_loader.py          # Carga y prepara datos NASA POWER
â”œâ”€â”€ neural_network.py       # Red neuronal con backpropagation
â”œâ”€â”€ train.py                # Script de entrenamiento
â”œâ”€â”€ test.py                 # Script de prueba/predicciÃ³n
â”œâ”€â”€ README.md               # Este archivo
â”‚
â”œâ”€â”€ southamerica_0_regional_monthly.csv  # Dataset (descargar)
â”‚
â””â”€â”€ resultados/             # GrÃ¡ficas generadas (creado automÃ¡ticamente)
    â”œâ”€â”€ curva_aprendizaje_y_predicciones.png
    â””â”€â”€ analisis_errores.png
```

---

## ğŸš€ InstalaciÃ³n

### Requisitos

- Python 3.7+
- Numpy
- Pandas
- Matplotlib
- Scikit-learn

### Instalar Dependencias

```bash
pip install numpy pandas matplotlib scikit-learn
```

---

## ğŸ“¥ Descargar el Dataset

1. Ve a: [https://huggingface.co/datasets/notadib/NASA-Power-Daily-Weather/](https://huggingface.co/datasets/notadib/NASA-Power-Daily-Weather/)
2. Descarga: `csvs/southamerica/southamerica_0_regional_monthly.csv`
3. Coloca el archivo en la carpeta del proyecto

**Alternativa (lÃ­nea de comandos):**
```bash
wget https://huggingface.co/datasets/notadib/NASA-Power-Daily-Weather/resolve/main/csvs/southamerica/southamerica_0_regional_monthly.csv
```

---

## ğŸ¯ Uso

### 1. Entrenar el Modelo

```bash
python train.py
```

**Esto harÃ¡:**
- âœ… CargarÃ¡ y prepararÃ¡ los datos
- âœ… CrearÃ¡ la red neuronal
- âœ… EntrenarÃ¡ por 1000 Ã©pocas
- âœ… GenerarÃ¡ visualizaciones
- âœ… GuardarÃ¡ el modelo entrenado

**Resultado esperado:**
```
ğŸ“Š MÃ‰TRICAS DE EVALUACIÃ“N
====================================================
ğŸ“ RaÃ­z del Error CuadrÃ¡tico Medio (RMSE):
   2.34Â°C
   â†’ En promedio, nos equivocamos Â±2.34Â°C

â­ Coeficiente de DeterminaciÃ³n (RÂ²):
   0.8523
   â†’ Bueno. El modelo captura la mayorÃ­a de patrones
```

### 2. Probar el Modelo

```bash
python test.py
```

**Opciones disponibles:**

#### a) PredicciÃ³n Interactiva
Ingresa datos manualmente:
```
Mes 1:
  T2M promedio: 20
  T2M_MAX: 25
  T2M_MIN: 15
Mes 2: ...
```

#### b) Predicciones sobre Dataset
Ve ejemplos reales de predicciones.

#### c) AnÃ¡lisis de Errores
Identifica dÃ³nde falla mÃ¡s el modelo.

#### d) ComparaciÃ³n con Baselines
Compara vs mÃ©todos simples.

---

## ğŸ§  CÃ³mo Funciona (Backpropagation)

### Algoritmo Simplificado

```python
for cada Ã©poca:
    for cada mini-batch:
        # 1. FORWARD PASS
        predicciÃ³n = calcular_salida(entrada)
        
        # 2. CALCULAR ERROR
        error = predicciÃ³n - valor_real
        
        # 3. BACKWARD PASS (Backpropagation)
        gradiente_salida = calcular_gradiente_capa_salida(error)
        gradiente_oculta = propagar_error_hacia_atras(gradiente_salida)
        
        # 4. ACTUALIZAR PESOS
        pesos -= learning_rate Ã— gradiente
```

### FÃ³rmulas Clave

**Forward Pass:**
```
z1 = X Â· W1 + b1
a1 = ReLU(z1)
z2 = a1 Â· W2 + b2
predicciÃ³n = z2
```

**Backward Pass:**
```
dz2 = predicciÃ³n - y_real
dW2 = a1^T Â· dz2
da1 = dz2 Â· W2^T
dz1 = da1 âŠ™ ReLU'(z1)
dW1 = X^T Â· dz1
```

**ActualizaciÃ³n:**
```
W = W - Î± Ã— dW
```
donde Î± = learning rate

---

## âš™ï¸ ConfiguraciÃ³n e HiperparÃ¡metros

En `train.py` puedes ajustar:

```python
HIDDEN_NEURONS = 16      # Neuronas en capa oculta
LEARNING_RATE = 0.001    # Tasa de aprendizaje
EPOCHS = 1000            # NÃºmero de Ã©pocas
BATCH_SIZE = 32          # TamaÃ±o del mini-batch
N_MONTHS_HISTORY = 3     # Meses de historia
```

### GuÃ­a de Ajuste

| Problema | SoluciÃ³n |
|----------|----------|
| PÃ©rdida muy alta | â†‘ Aumentar neuronas ocultas<br>â†‘ Aumentar Ã©pocas |
| No converge | â†“ Reducir learning rate |
| Converge muy lento | â†‘ Aumentar learning rate |
| Overfitting | â†“ Reducir neuronas ocultas<br>Agregar mÃ¡s datos |

---

## ğŸ“Š InterpretaciÃ³n de Resultados

### MÃ©tricas

| MÃ©trica | Significado | Valor Bueno |
|---------|-------------|-------------|
| **RMSE** | Error tÃ­pico en Â°C | < 3.0Â°C |
| **MAE** | Error absoluto promedio | < 2.5Â°C |
| **RÂ²** | % de variaciÃ³n explicada | > 0.7 |

### GrÃ¡ficas

#### 1. Curva de Aprendizaje
- **Descendente**: âœ… El modelo aprende
- **Plana muy alta**: âŒ No puede aprender (underfitting)
- **Oscilatoria**: âš ï¸ Learning rate muy alto

#### 2. Predicciones vs Reales
- **Puntos cerca de lÃ­nea**: âœ… Buenas predicciones
- **Puntos dispersos**: âŒ Predicciones inconsistentes
- **PatrÃ³n sistemÃ¡tico**: âš ï¸ Sesgo en el modelo

#### 3. DistribuciÃ³n de Errores
- **Centrada en 0**: âœ… Sin sesgo
- **Forma de campana**: âœ… Errores aleatorios
- **Desplazada**: âŒ Modelo sobre/subestima

---

## ğŸ“ Conceptos Aprendidos

### MatemÃ¡ticas Implementadas

- âœ… MultiplicaciÃ³n de matrices
- âœ… Regla de la cadena (cÃ¡lculo)
- âœ… Derivadas parciales
- âœ… Gradient Descent
- âœ… FunciÃ³n ReLU y su derivada

### Machine Learning

- âœ… Forward/Backward propagation
- âœ… Mini-batch training
- âœ… NormalizaciÃ³n de datos
- âœ… Train/Test split
- âœ… MÃ©tricas de evaluaciÃ³n
- âœ… Baselines de comparaciÃ³n

### Buenas PrÃ¡cticas

- âœ… CÃ³digo modular y documentado
- âœ… Manejo de errores
- âœ… Visualizaciones informativas
- âœ… Reproducibilidad (random_seed)

---

## ğŸ”§ SoluciÃ³n de Problemas

### Error: "No se encontrÃ³ el archivo"
```
âŒ ERROR: No se encontrÃ³ el archivo southamerica_0_regional_monthly.csv
```
**SoluciÃ³n:** Descarga el dataset de HuggingFace (ver secciÃ³n "Descargar el Dataset")

### Error: "No such file or directory: 'modelo_temperatura.pkl'"
**SoluciÃ³n:** Primero ejecuta `train.py` para entrenar y guardar un modelo

### PÃ©rdida no disminuye
**Posibles causas:**
- Learning rate muy alto â†’ Reducir a 0.0001
- Datos no normalizados â†’ Verificar que DataLoader normaliza
- Arquitectura inadecuada â†’ Probar con mÃ¡s/menos neuronas

### Predicciones siempre iguales
**Posibles causas:**
- Pesos inicializados en cero â†’ El cÃ³digo ya usa inicializaciÃ³n aleatoria
- Learning rate muy bajo â†’ Aumentar a 0.01
- ConvergiÃ³ a mÃ­nimo local â†’ Reiniciar con diferente random_seed

---

## ğŸ“š Referencias

### Dataset
- **NASA POWER**: [https://power.larc.nasa.gov/](https://power.larc.nasa.gov/)
- **HuggingFace Dataset**: [https://huggingface.co/datasets/notadib/NASA-Power-Daily-Weather](https://huggingface.co/datasets/notadib/NASA-Power-Daily-Weather)

### TeorÃ­a
- **Backpropagation**: Rumelhart, Hinton & Williams (1986)
- **ReLU**: Nair & Hinton (2010)
- **Batch Normalization**: Ioffe & Szegedy (2015)

### LibrerÃ­as
- **NumPy**: [https://numpy.org/](https://numpy.org/)
- **Pandas**: [https://pandas.pydata.org/](https://pandas.pydata.org/)
- **Matplotlib**: [https://matplotlib.org/](https://matplotlib.org/)

---

## ğŸš€ PrÃ³ximos Pasos

### Mejoras Posibles

1. **Agregar mÃ¡s caracterÃ­sticas**
   - PrecipitaciÃ³n
   - Humedad
   - PresiÃ³n atmosfÃ©rica

2. **Arquitectura mÃ¡s compleja**
   - MÃºltiples capas ocultas
   - Dropout para regularizaciÃ³n
   - Batch normalization

3. **OptimizaciÃ³n avanzada**
   - Adam optimizer
   - Learning rate decay
   - Early stopping

4. **ValidaciÃ³n cruzada**
   - K-fold cross-validation
   - Time series split

5. **ComparaciÃ³n con librerÃ­as**
   - Implementar en TensorFlow
   - Implementar en PyTorch
   - Comparar rendimiento

---

## ğŸ“ Notas

- **Tiempo de entrenamiento**: ~2-5 minutos en CPU moderna
- **PrecisiÃ³n esperada**: RMSE entre 2-4Â°C
- **Dataset size**: ~38 MB
- **Modelo guardado**: ~50-100 KB

---

## ğŸ‘¨â€ğŸ’» Autor

Proyecto educativo para aprender backpropagation desde cero.

---

## ğŸ“„ Licencia

Este proyecto es de cÃ³digo abierto y estÃ¡ disponible para fines educativos.

---

## ğŸ™ Agradecimientos

- NASA POWER por los datos meteorolÃ³gicos
- HuggingFace por hospedar el dataset
- Comunidad de Machine Learning por recursos educativos

---

**Â¡Happy Learning! ğŸ“ğŸš€**